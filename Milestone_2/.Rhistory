library(C50)
#library(gmodels)
library(randomForest)
library(gridExtra)
library(gbm)
# Read the dataset
data <- read.csv("datasets/cleaned_dataset.csv")
# Remove rows with missing values
data <- na.omit(data)
# Select relevant columns
data <- data[, c("t_cap", "t_hh", "t_rd", "t_rsa", "t_ttlh")]
# Convert t_cap to categories
data <- data %>%
mutate(t_cap_category = cut(t_cap, breaks = c(-Inf, 1500, 1800, Inf), labels = c("low", "medium", "high")))
# Define proportion for training data
train_proportion <- 0.7
# Generate random indices to shuffle the dataset
set.seed(123)  # for reproducibility
random_indices <- sample(nrow(data))
# Shuffle the dataset
shuffled_data <- data[random_indices, ]
# Split the shuffled dataset into training and testing sets
train_data <- shuffled_data[1:round(train_proportion * nrow(shuffled_data)), ]
test_data <- shuffled_data[(round(train_proportion * nrow(shuffled_data)) + 1):nrow(shuffled_data), ]
## Part A: Show that the distribution after the split is similar to the original.
# Visualize distributions
original_plot <- ggplot(data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Original Dataset", x = "Turbine Capacity (kW)", y = "Frequency")
train_plot <- ggplot(train_data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Training Set", x = "Turbine Capacity (kW)", y = "Frequency")
test_plot <- ggplot(test_data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Testing Set", x = "Turbine Capacity (kW)", y = "Frequency")
# Combine the plots
png(filename = "generated_graphs/distribution_plots.png", width = 800, height = 600)
grid.arrange(original_plot, train_plot, test_plot, ncol = 3)
dev.off()
summary(train_data)
summary(test_data)
summary(data)
## Part B: Train a decision tree
# Build the C5.0 model
model <- C5.0(train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], train_data$t_cap_category)
summary(model)
# Open a PNG device for graphics output
png("generated_graphs/decision_tree.png", width = 800, height = 600)
# Plot the decision tree
plot(model)
# Close the device and save the plot
dev.off()
# Confusion matrix for the training set
train_predict <- predict(model, train_data)
train_confusion <- CrossTable(train_data$t_cap_category, train_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
train_accuracy <- mean(train_predict == train_data$t_cap_category) * 100
# Confusion matrix for the testing set
test_predict <- predict(model, test_data)
test_confusion <- CrossTable(test_data$t_cap_category, test_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
test_accuracy <- mean(test_predict == test_data$t_cap_category) * 100
cat("Decision Tree Training Accuracy:", train_accuracy, "%\n")
cat("Decision Tree Testing Accuracy:", test_accuracy, "%\n")
## Part C: Apply boosting with different numbers of trees
# Train boosting models with different numbers of trees
boost_model_10trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10, distribution = "multinomial")
boost_model_50trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50, distribution = "multinomial")
# Make predictions on the test set
boost_pred_10trees <- predict(boost_model_10trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10)
boost_pred_50trees <- predict(boost_model_50trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50)
# Convert predicted probabilities to class labels
boost_pred_10trees_class <- colnames(boost_pred_10trees)[apply(boost_pred_10trees, 1, which.max)]
boost_pred_50trees_class <- colnames(boost_pred_50trees)[apply(boost_pred_50trees, 1, which.max)]
# Calculate accuracy
boost_accuracy_10trees <- mean(boost_pred_10trees_class == test_data$t_cap_category) * 100
boost_accuracy_50trees <- mean(boost_pred_50trees_class == test_data$t_cap_category) * 100
cat("Boosting Accuracy (10 trees):", boost_accuracy_10trees, "%\n")
cat("Boosting Accuracy (50 trees):", boost_accuracy_50trees, "%\n")
## Part D: Train bagging and random forests with different numbers of trees
# Bagging
bagging_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
bagging_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
bagging_predict <- predict(bagging_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
bagging_accuracy[i] <- mean(bagging_predict == test_data$t_cap_category) * 100
cat("Bagging Accuracy (", i, "trees): ", bagging_accuracy[i], "%\n")
}
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
rf_predict <- predict(rf_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy[i] <- mean(rf_predict == test_data$t_cap_category) * 100
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[i], "%\n")
}
# Identify most important features in each random forest and save plots
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
varImpPlot(rf_model)
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
varImpPlot(rf_model)
dev.off()
}
# Load required libraries
library(ggplot2)
library(dplyr)
library(C50)
#library(gmodels)
library(randomForest)
library(gridExtra)
library(gbm)
# Read the dataset
data <- read.csv("datasets/cleaned_dataset.csv")
# Remove rows with missing values
data <- na.omit(data)
# Select relevant columns
data <- data[, c("t_cap", "t_hh", "t_rd", "t_rsa", "t_ttlh")]
# Convert t_cap to categories
data <- data %>%
mutate(t_cap_category = cut(t_cap, breaks = c(-Inf, 1500, 1800, Inf), labels = c("low", "medium", "high")))
# Define proportion for training data
train_proportion <- 0.7
# Generate random indices to shuffle the dataset
set.seed(123)  # for reproducibility
random_indices <- sample(nrow(data))
# Shuffle the dataset
shuffled_data <- data[random_indices, ]
# Split the shuffled dataset into training and testing sets
train_data <- shuffled_data[1:round(train_proportion * nrow(shuffled_data)), ]
test_data <- shuffled_data[(round(train_proportion * nrow(shuffled_data)) + 1):nrow(shuffled_data), ]
## Part A: Show that the distribution after the split is similar to the original.
# Visualize distributions
original_plot <- ggplot(data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Original Dataset", x = "Turbine Capacity (kW)", y = "Frequency")
train_plot <- ggplot(train_data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Training Set", x = "Turbine Capacity (kW)", y = "Frequency")
test_plot <- ggplot(test_data, aes(x = t_cap)) +
geom_histogram(fill = "blue", color = "black", bins = 20) +
labs(title = "Distribution of Testing Set", x = "Turbine Capacity (kW)", y = "Frequency")
# Combine the plots
png(filename = "generated_graphs/distribution_plots.png", width = 800, height = 600)
grid.arrange(original_plot, train_plot, test_plot, ncol = 3)
dev.off()
summary(train_data)
summary(test_data)
summary(data)
## Part B: Train a decision tree
# Build the C5.0 model
model <- C5.0(train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], train_data$t_cap_category)
summary(model)
# Open a PNG device for graphics output
png("generated_graphs/decision_tree.png", width = 800, height = 600)
# Plot the decision tree
plot(model)
# Close the device and save the plot
dev.off()
# Confusion matrix for the training set
train_predict <- predict(model, train_data)
train_confusion <- CrossTable(train_data$t_cap_category, train_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
train_accuracy <- mean(train_predict == train_data$t_cap_category) * 100
# Confusion matrix for the testing set
test_predict <- predict(model, test_data)
test_confusion <- CrossTable(test_data$t_cap_category, test_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
test_accuracy <- mean(test_predict == test_data$t_cap_category) * 100
cat("Decision Tree Training Accuracy:", train_accuracy, "%\n")
cat("Decision Tree Testing Accuracy:", test_accuracy, "%\n")
## Part C: Apply boosting with different numbers of trees
# Train boosting models with different numbers of trees
boost_model_10trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10, distribution = "multinomial")
boost_model_50trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50, distribution = "multinomial")
# Make predictions on the test set
boost_pred_10trees <- predict(boost_model_10trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10)
boost_pred_50trees <- predict(boost_model_50trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50)
# Convert predicted probabilities to class labels
boost_pred_10trees_class <- colnames(boost_pred_10trees)[apply(boost_pred_10trees, 1, which.max)]
boost_pred_50trees_class <- colnames(boost_pred_50trees)[apply(boost_pred_50trees, 1, which.max)]
# Calculate accuracy
boost_accuracy_10trees <- mean(boost_pred_10trees_class == test_data$t_cap_category) * 100
boost_accuracy_50trees <- mean(boost_pred_50trees_class == test_data$t_cap_category) * 100
cat("Boosting Accuracy (10 trees):", boost_accuracy_10trees, "%\n")
cat("Boosting Accuracy (50 trees):", boost_accuracy_50trees, "%\n")
## Part D: Train bagging and random forests with different numbers of trees
# Bagging
bagging_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
bagging_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
bagging_predict <- predict(bagging_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
bagging_accuracy[i] <- mean(bagging_predict == test_data$t_cap_category) * 100
cat("Bagging Accuracy (", i, "trees): ", bagging_accuracy[i], "%\n")
}
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
rf_predict <- predict(rf_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy[i] <- mean(rf_predict == test_data$t_cap_category) * 100
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[i], "%\n")
}
# Identify most important features in each random forest and save plots
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
varImpPlot(rf_model)
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
varImpPlot(rf_model)
dev.off()
}
# Identify most important features in each random forest and save plots
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
# Create variable importance plot
varImp <- varImpPlot(rf_model, main = paste("Variable Importance Plot (", i, " trees)"), ylab = "Mean Decrease in Accuracy", xlab = "Features")
# Save the plot as a PNG file
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
print(varImp)
dev.off()
}
library(gridExtra)
library(randomForest)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model, main = paste("Variable Importance Plot (", i, " trees)"), ylab = "Mean Decrease in Accuracy", xlab = "Features")
}
library(randomForest)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"),
ylab = "Mean Decrease in Accuracy")
}
library(randomForest)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
library(randomForest)
library(gridExtra)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
library(randomForest)
library(gridExtra)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
library(randomForest)
library(gridExtra)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
library(randomForest)
library(gridExtra)
# Create an empty list to store the plots
varImpPlots <- list()
# Set margins for the plot
par(mar = c(5, 4, 4, 2) + 0.1)
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
library(randomForest)
library(gridExtra)
# Create an empty list to store the plots
varImpPlots <- list()
# Set smaller margins for the plot
par(mar = c(2, 2, 2, 2))
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
varImpPlots[[i]] <- varImpPlot(rf_model,
main = paste("Variable Importance Plot (", i, " trees)"))
}
# Arrange the plots using grid.arrange
combined_plot <- grid.arrange(grobs = varImpPlots, ncol = 2)
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
rf_predict <- predict(rf_model,
newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy[i] <- mean(rf_predict == test_data$t_cap_category) * 100
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[i], "%\n")
}
# Plotting the accuracy values
barplot(rf_accuracy,
names.arg = c(10, 50, 100, 500),
main = "Random Forest Accuracy vs. Number of Trees",
xlab = "Number of Trees",
ylab = "Accuracy (%)")
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
rf_predict <- predict(rf_model,
newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy[i] <- mean(rf_predict == test_data$t_cap_category) * 100
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[i], "%\n")
}
# Plotting the accuracy values
barplot(rf_accuracy,
names.arg = c("10", "50", "100", "500"), # Convert to character type
main = "Random Forest Accuracy vs. Number of Trees",
xlab = "Number of Trees",
ylab = "Accuracy (%)")
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
rf_predict <- predict(rf_model,
newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy <- c(rf_accuracy, mean(rf_predict == test_data$t_cap_category) * 100)
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[length(rf_accuracy)], "%\n")
}
# Plotting the accuracy values
barplot(rf_accuracy,
names.arg = c("10", "50", "100", "500"),
main = "Random Forest Accuracy vs. Number of Trees",
xlab = "Number of Trees",
ylab = "Accuracy (%)")
# Identify most important features in each random forest and save plots
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
varImpPlot(rf_model)
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
varImpPlot(rf_model)
dev.off()
}
# Identify most important features in each random forest and save plots
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlot(rf_model, main = plot_title)
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
varImpPlot(rf_model, main = plot_title)
dev.off()
}
library(gridExtra)
library(randomForest)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlots[[i]] <- varImpPlot(rf_model, main = plot_title)
}
# Combine plots using grid.arrange
combined_plot <- do.call(grid.arrange, varImpPlots)
library(gridExtra)
library(randomForest)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlots[[i]] <- varImpPlot(rf_model, main = plot_title)
}
# Combine plots using grid.arrange
combined_plot <- grid.arrange(grobs = varImpPlots, ncol = 2)
library(gridExtra)
library(randomForest)
library(ggplot2)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlots[[i]] <- ggplotGrob(varImpPlot(rf_model, main = plot_title))
}
library(gridExtra)
library(randomForest)
library(ggplot2)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlots[[i]] <- ggplotGrob(varImpPlot(rf_model, main = plot_title))
}
library(gridExtra)
library(randomForest)
library(ggplot2)
# Create an empty list to store the plots
varImpPlots <- list()
# Create variable importance plots for different numbers of trees
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ .,
data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")],
ntree = i)
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlots[[i]] <- ggplotGrob(varImpPlot(rf_model, main = plot_title))
}
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
cat("Random Forest with", i, "trees: \n")
plot_title <- paste("Variable Importance Plot (", i, " trees)")
varImpPlot(rf_model, main = plot_title)
png(filename = paste("generated_graphs/random_forest_", i, "_trees.png", sep = ""), width = 800, height = 600)
varImpPlot(rf_model, main = plot_title)
dev.off()
}
# Confusion matrix for the training set
train_predict <- predict(model, train_data)
train_confusion <- CrossTable(train_data$t_cap_category, train_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
train_accuracy <- mean(train_predict == train_data$t_cap_category) * 100
# Confusion matrix for the testing set
test_predict <- predict(model, test_data)
test_confusion <- CrossTable(test_data$t_cap_category, test_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual', 'predicted'))
test_accuracy <- mean(test_predict == test_data$t_cap_category) * 100
cat("Decision Tree Training Accuracy:", train_accuracy, "%\n")
cat("Decision Tree Testing Accuracy:", test_accuracy, "%\n")
## Part C: Apply boosting with different numbers of trees
# Train boosting models with different numbers of trees
boost_model_10trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10, distribution = "multinomial")
boost_model_50trees <- gbm(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50, distribution = "multinomial")
# Make predictions on the test set
boost_pred_10trees <- predict(boost_model_10trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 10)
boost_pred_50trees <- predict(boost_model_50trees, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], n.trees = 50)
# Convert predicted probabilities to class labels
boost_pred_10trees_class <- colnames(boost_pred_10trees)[apply(boost_pred_10trees, 1, which.max)]
boost_pred_50trees_class <- colnames(boost_pred_50trees)[apply(boost_pred_50trees, 1, which.max)]
# Calculate accuracy
boost_accuracy_10trees <- mean(boost_pred_10trees_class == test_data$t_cap_category) * 100
boost_accuracy_50trees <- mean(boost_pred_50trees_class == test_data$t_cap_category) * 100
cat("Boosting Accuracy (10 trees):", boost_accuracy_10trees, "%\n")
cat("Boosting Accuracy (50 trees):", boost_accuracy_50trees, "%\n")
## Part D: Train bagging and random forests with different numbers of trees
# Bagging
bagging_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
bagging_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
bagging_predict <- predict(bagging_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
bagging_accuracy[i] <- mean(bagging_predict == test_data$t_cap_category) * 100
cat("Bagging Accuracy (", i, "trees): ", bagging_accuracy[i], "%\n")
}
# Random Forest
rf_accuracy <- numeric()
for (i in c(10, 50, 100, 500)) {
rf_model <- randomForest(train_data$t_cap_category ~ ., data = train_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")], ntree = i)
rf_predict <- predict(rf_model, newdata = test_data[, c("t_hh", "t_rd", "t_rsa", "t_ttlh")])
rf_accuracy[i] <- mean(rf_predict == test_data$t_cap_category) * 100
cat("Random Forest Accuracy (", i, "trees): ", rf_accuracy[i], "%\n")
}
